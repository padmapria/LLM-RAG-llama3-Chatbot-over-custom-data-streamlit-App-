{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccce3110",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T08:56:30.341718Z",
     "start_time": "2024-06-18T08:56:30.319943Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.schema.document import Document\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.chains.history_aware_retriever import create_history_aware_retriever\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables. Assumes that project contains .env file with API keys\n",
    "load_dotenv()\n",
    "#---- Set OpenAI API key \n",
    "# Change environment variable name from \"OPENAI_API_KEY\" to the name given in \n",
    "# your .env file.\n",
    "#openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "CHROMA_PATH = \"chroma\"\n",
    "DATA_PATH = \"data/books\"\n",
    "cached_llm = Ollama(model=\"llama3\")\n",
    "\n",
    "aw_prompt = PromptTemplate.from_template(\n",
    "    \"\"\" \n",
    "    <s>[INST] You are a AI assistant good at searching docuemnts. If you do not have an answer from the provided information say so. [/INST] </s>\n",
    "    [INST] {input}\n",
    "           Context: {context}\n",
    "           Answer:\n",
    "    [/INST]\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "raw_prompt = PromptTemplate.from_template(\n",
    "\"\"\"\n",
    "<s>[INST] You are an AI assistant skilled at searching documents. You should only provide answers based on the information contained in the provided documents. If the information is not available in the documents, state that you do not have an answer. [/INST] </s>\n",
    "[INST] {input}\n",
    "Context: {context}\n",
    "Answer:\n",
    "[/INST]\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d63ac732",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T08:44:17.974187Z",
     "start_time": "2024-06-18T08:43:46.789444Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44125af521d947c5af9716d217c6edd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<langchain_community.document_loaders.directory.DirectoryLoader object at 0x000002805C574500>\n",
      "Split 1 documents into 698 chunks.\n",
      "4\n",
      "\n",
      "1.1 Performance Estimation: Generalization Performance vs. Model Selection . . . . .\n",
      "\n",
      "4\n",
      "\n",
      "1.2 Assumptions and Terminology . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "\n",
      "5\n",
      "\n",
      "1.3 Resubstitution Validation and the Holdout Method . . . . . . . . . . . . . . . . . .\n",
      "\n",
      "7\n",
      "\n",
      "1.4 StratiÔ¨Åcation .\n",
      "{'source': 'data\\\\books\\\\Model Evaluation, Model Selection, and Algorithm in ML.pdf', 'start_index': 1915}\n",
      "Saved 698 chunks to chroma_new.\n"
     ]
    }
   ],
   "source": [
    "embedding = FastEmbedEmbeddings()\n",
    "\n",
    "def generate_data_store():\n",
    "    documents = load_documents()\n",
    "    chunks = split_text(documents)\n",
    "    save_to_chroma(chunks)\n",
    "\n",
    "\n",
    "def load_documents():\n",
    "    loader = DirectoryLoader(DATA_PATH, glob=\"*.pdf\")\n",
    "    print(loader)\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "\n",
    "def split_text(documents: list[Document]):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=300,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len,\n",
    "        add_start_index=True,\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(chunks)} chunks.\")\n",
    "\n",
    "    document = chunks[10]\n",
    "    print(document.page_content)\n",
    "    print(document.metadata)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def save_to_chroma(chunks: list[Document]):\n",
    "    # Clear out the database first.\n",
    "    if os.path.exists(CHROMA_PATH):\n",
    "        shutil.rmtree(CHROMA_PATH)\n",
    "\n",
    "    # Create a new DB from the documents.\n",
    "    db = Chroma.from_documents(\n",
    "        chunks, embedding, persist_directory=CHROMA_PATH\n",
    "    )\n",
    "    db.persist()\n",
    "    print(f\"Saved {len(chunks)} chunks to {CHROMA_PATH}.\")\n",
    "    \n",
    "generate_data_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68bbe3a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T08:56:35.250293Z",
     "start_time": "2024-06-18T08:56:35.230513Z"
    }
   },
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "\n",
    "def retrieve_t_chroma(query):\n",
    "    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding)\n",
    "    retriever = db.as_retriever(\n",
    "            search_type=\"similarity_score_threshold\",\n",
    "            search_kwargs={\n",
    "                \"k\": 20,\n",
    "                \"score_threshold\": 0.5,\n",
    "            },\n",
    "        )\n",
    "    \n",
    "    retriever_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "            (\n",
    "                \"human\",\n",
    "                \"Given the above conversation, generate a search query to lookup in order to get information relevant to the conversation\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    history_aware_retriever = create_history_aware_retriever(\n",
    "        llm=cached_llm, retriever=retriever, prompt=retriever_prompt\n",
    "    )\n",
    "\n",
    "    document_chain = create_stuff_documents_chain(cached_llm, raw_prompt)\n",
    "    # chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "    retrieval_chain = create_retrieval_chain(\n",
    "        # retriever,\n",
    "        history_aware_retriever,\n",
    "        document_chain,\n",
    "    )\n",
    "\n",
    "    # result = chain.invoke({\"input\": query})\n",
    "    result = retrieval_chain.invoke({\"input\": query})\n",
    "    print(\"Answer **:: \",result[\"answer\"])\n",
    "    chat_history.append(HumanMessage(content=query))\n",
    "    chat_history.append(AIMessage(content=result[\"answer\"]))\n",
    "\n",
    "    sources = []\n",
    "    for doc in result[\"context\"]:\n",
    "        sources.append(\n",
    "            {\"source\": doc.metadata[\"source\"], \"page_content\": doc.page_content}\n",
    "        )\n",
    "\n",
    "    response_answer = {\"answer\": result[\"answer\"], \"sources\": sources}\n",
    "    print(response_answer)\n",
    "    print(\"!!!!!!!!!!!!!!!!!!\")\n",
    "    return chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71ea3f22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T08:58:44.466966Z",
     "start_time": "2024-06-18T08:56:38.389625Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer **::  According to the provided documents, model evaluation refers to the process of estimating and assessing the performance of a machine learning model. This involves techniques for estimating the uncertainty of estimated model performance as well as the model's variance and stability. Model evaluation is an essential part of the machine learning pipeline, along with model selection and algorithm selection.\n",
      "\n",
      "Model evaluation is used to assess the predictive performance of a model and provides insights into the model's generalization ability, bias-variance trade-off, and uncertainty estimates. It also enables comparison between different models or algorithms to determine which one performs best.\n",
      "\n",
      "The document highlights several techniques for model evaluation, including k-fold cross-validation, holdout method, and various statistical tests. These techniques help ensure that the evaluated performance of a model is accurate and reliable, allowing researchers and practitioners to make informed decisions about their machine learning projects.\n",
      "{'answer': \"According to the provided documents, model evaluation refers to the process of estimating and assessing the performance of a machine learning model. This involves techniques for estimating the uncertainty of estimated model performance as well as the model's variance and stability. Model evaluation is an essential part of the machine learning pipeline, along with model selection and algorithm selection.\\n\\nModel evaluation is used to assess the predictive performance of a model and provides insights into the model's generalization ability, bias-variance trade-off, and uncertainty estimates. It also enables comparison between different models or algorithms to determine which one performs best.\\n\\nThe document highlights several techniques for model evaluation, including k-fold cross-validation, holdout method, and various statistical tests. These techniques help ensure that the evaluated performance of a model is accurate and reliable, allowing researchers and practitioners to make informed decisions about their machine learning projects.\", 'sources': [{'source': 'data\\\\books\\\\Model Evaluation, Model Selection, and Algorithm in ML.pdf', 'page_content': 'Model evaluation is certainly not just the end point of our machine learning pipeline. Before we handle any data, we want to plan ahead and use techniques that are suited for our purposes. In this article, we will go over a selection of these techniques, and we will see how they Ô¨Åt into the bigger'}, {'source': 'data\\\\books\\\\Model Evaluation, Model Selection, and Algorithm in ML.pdf', 'page_content': '45\\n\\n4.15 Conclusions .\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\\n46\\n\\n4.16 Acknowledgments\\n\\n.\\n\\n.\\n\\n.\\n\\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n\\n48\\n\\n3\\n\\n1\\n\\nIntroduction: Essential Model Evaluation Terms and Techniques'}, {'source': 'data\\\\books\\\\Model Evaluation, Model Selection, and Algorithm in ML.pdf', 'page_content': 'The previous section (Section 1, Introduction: Essential Model Evaluation Terms and Techniques) introduced the general ideas behind model evaluation in supervised machine learning. We discussed the holdout method, which helps us to deal with real world limitations such as limited access to new,'}, {'source': 'data\\\\books\\\\Model Evaluation, Model Selection, and Algorithm in ML.pdf', 'page_content': 'This section introduces some of the advanced techniques for model evaluation. We will start by discussing techniques for estimating the uncertainty of our estimated model performance as well as the model‚Äôs variance and stability. And after getting these basics under our belt, we will look at'}, {'source': 'data\\\\books\\\\Model Evaluation, Model Selection, and Algorithm in ML.pdf', 'page_content': '1.2 Assumptions and Terminology\\n\\nModel evaluation is certainly a complex topic. To make sure that we do not diverge too much from the core message, let us make certain assumptions and go over some of the technical terms that we will use throughout this article.'}, {'source': 'data\\\\books\\\\Model Evaluation, Model Selection, and Algorithm in ML.pdf', 'page_content': ']\\n\\nG L . s c [\\n\\n3 v 8 0 8 2 1 . 1 1 8 1 : v i X r a\\n\\nContents\\n\\n1 Introduction: Essential Model Evaluation Terms and Techniques\\n\\n4\\n\\n1.1 Performance Estimation: Generalization Performance vs. Model Selection . . . . .\\n\\n4'}, {'source': 'data\\\\books\\\\Model Evaluation, Model Selection, and Algorithm in ML.pdf', 'page_content': 'Note that Ô¨Åtting the model on all available data might yield a model that is likely slightly different from the model evaluated in Step 5. However, in theory, using all data (that is, training and test data) to Ô¨Åt the model should only improve its performance. Under this assumption, the evaluated'}, {'source': 'data\\\\books\\\\Model Evaluation, Model Selection, and Algorithm in ML.pdf', 'page_content': 'The correct use of model evaluation, model selection, and algorithm selection techniques is vital in academic machine learning research as well as in many industrial settings. This article reviews different techniques that can be used for each of these three subtasks and discusses the main'}, {'source': 'data\\\\books\\\\Model Evaluation, Model Selection, and Algorithm in ML.pdf', 'page_content': 'estimates. After covering the basics of model evaluation in this and the previous section, the next section introduces hyperparameter tuning and model selection.'}, {'source': 'data\\\\books\\\\Model Evaluation, Model Selection, and Algorithm in ML.pdf', 'page_content': 'from Section 1, there are three related, yet different tasks or reasons why we care about model evaluation:'}, {'source': 'data\\\\books\\\\Model Evaluation, Model Selection, and Algorithm in ML.pdf', 'page_content': 'model selections allows us to keep the test set \"independent\" for model evaluation. Once more, let us recall the \"3 goals\" of performance estimation:'}, {'source': 'data\\\\books\\\\Model Evaluation, Model Selection, and Algorithm in ML.pdf', 'page_content': 'Figure 23: A recommended subset of techniques to be used to address different aspects of model evaluation in the context of small and large datasets. The abbreviation \"MC\" stands for \"Model Comparison,\" and \"AC\" stands for \"Algorithm Comparison,\" to distinguish these two tasks.'}, {'source': 'data\\\\books\\\\Model Evaluation, Model Selection, and Algorithm in ML.pdf', 'page_content': 'Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning\\n\\nSebastian Raschka University of Wisconsin‚ÄìMadison Department of Statistics November 2018 sraschka@wisc.edu\\n\\n0 2 0 2\\n\\nv o N 1 1\\n\\nAbstract'}, {'source': 'data\\\\books\\\\Model Evaluation, Model Selection, and Algorithm in ML.pdf', 'page_content': 'performance of a model. The bias-variance trade-off was discussed in the context of estimating the generalization performance as well as methods for computing the uncertainty of performance estimates. This third section focusses on different methods of cross-validation for model evaluation and'}, {'source': 'data\\\\books\\\\Model Evaluation, Model Selection, and Algorithm in ML.pdf', 'page_content': 'It is about time to introduce the probably most common technique for model evaluation and model selection in machine learning practice: k-fold cross-validation. The term cross-validation is used loosely in literature, where practitioners and researchers sometimes refer to the train/test holdout'}, {'source': 'data\\\\books\\\\Model Evaluation, Model Selection, and Algorithm in ML.pdf', 'page_content': 'model evaluation, model selection, and algorithm selection) in the context of statistical tests. The conclusions that can be drawn from empirical comparison on simulated datasets are summarized below.'}, {'source': 'data\\\\books\\\\Model Evaluation, Model Selection, and Algorithm in ML.pdf', 'page_content': 'More often than not, we want to compare different algorithms to each other, oftentimes in terms of predictive and computational performance. Let us summarize the main points why we evaluate the predictive performance of a model:'}, {'source': 'data\\\\books\\\\Model Evaluation, Model Selection, and Algorithm in ML.pdf', 'page_content': 'approach such situations where we want to compare multiple models to each other [Westfall et al., 2010].'}, {'source': 'data\\\\books\\\\Model Evaluation, Model Selection, and Algorithm in ML.pdf', 'page_content': 'data) to Ô¨Åt the model should only improve its performance. Under this assumption, the evaluated performance from Step 5 might slightly underestimate the performance of the model Ô¨Åtted in Step 6. (If we use test data for Ô¨Åtting, we do not have data left to evaluate the model, unless we collect new'}, {'source': 'data\\\\books\\\\Model Evaluation, Model Selection, and Algorithm in ML.pdf', 'page_content': 'we use test data for Ô¨Åtting, we do not have data left to evaluate the model, unless we collect new data.) In real-world applications, having the \"best possible\" model is often desired ‚Äì or in other words, we do not mind if we slightly underestimated its performance. In any case, we can regard this'}]}\n",
      "!!!!!!!!!!!!!!!!!!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='what is model Evaluation?'),\n",
       " AIMessage(content=\"According to the provided documents, model evaluation refers to the process of estimating and assessing the performance of a machine learning model. This involves techniques for estimating the uncertainty of estimated model performance as well as the model's variance and stability. Model evaluation is an essential part of the machine learning pipeline, along with model selection and algorithm selection.\\n\\nModel evaluation is used to assess the predictive performance of a model and provides insights into the model's generalization ability, bias-variance trade-off, and uncertainty estimates. It also enables comparison between different models or algorithms to determine which one performs best.\\n\\nThe document highlights several techniques for model evaluation, including k-fold cross-validation, holdout method, and various statistical tests. These techniques help ensure that the evaluated performance of a model is accurate and reliable, allowing researchers and practitioners to make informed decisions about their machine learning projects.\")]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"what is model Evaluation?\"\n",
    "retrieve_t_chroma(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2414632",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
